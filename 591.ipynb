{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c3377903fe7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_html\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Derek/anaconda/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Derek/anaconda/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;31m# By explicitly closing the session, we avoid leaving sockets open which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# can trigger a ResourceWarning in some cases, and look like a memory leak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Derek/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    463\u001b[0m         }\n\u001b[1;32m    464\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Derek/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Derek/anaconda/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    368\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m                 )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Derek/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, **response_kw)\u001b[0m\n\u001b[1;32m    542\u001b[0m             httplib_response = self._make_request(conn, method, url,\n\u001b[1;32m    543\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                                                   body=body, headers=headers)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Derek/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Derek/anaconda/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Derek/anaconda/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Derek/anaconda/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Derek/anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#台北市URL\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from math import ceil\n",
    "f = open('591_4.txt','w')\n",
    "page_html = \"http://sale.591.com.tw/index.php?module=search&action=rslist&is_new_list=1&type=2&searchtype=1&region=1&orderType=desc&listview=img&shType=list&kind=9&firstRow={0}&totalRows=11678\"\n",
    "res = requests.get(page_html)\n",
    "count = json.loads(res.text)\n",
    "cases = ''.join(count['count'].split(','))\n",
    "pages = int(ceil(float(cases)/20))\n",
    "for i in range(0,pages+1):\n",
    "    i = 20 * i\n",
    "    res = requests.get(page_html.format(i))\n",
    "    data = json.loads(res.text)\n",
    "    soup = bs(data['main'])\n",
    "    for ele in soup.select('.shList'):\n",
    "        f.write(\"http://sale.591.com.tw/\"+ele.select('a')[0]['href']+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7239\n"
     ]
    }
   ],
   "source": [
    "#計算URL\n",
    "line_num = 0\n",
    "f = open(\"housefun_detail.txt\",\"r\")\n",
    "for line in f.readlines():\n",
    "    line_num = line_num + 1\n",
    "print line_num\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#將重複的物件讀出\n",
    "case_dic = {}\n",
    "f = open('591_2_all.txt','r')\n",
    "re = open('re_2.txt','w')\n",
    "for line in f.readlines():\n",
    "    rec = line.strip()\n",
    "    caseno = rec.split('/')[3]\n",
    "    if caseno not in case_dic:\n",
    "        case_dic[caseno] = 1\n",
    "    else:\n",
    "        re.write(caseno+'\\n')\n",
    "re.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#存入各物件html+經緯度\n",
    "dir = '591xinbei'\n",
    "f = open('591_2.txt','r')\n",
    "for line in f:\n",
    "    file_name = line.strip().split('/')[3].split('.')[0].split('-')[2]\n",
    "    input_file = open(dir+'/'+file_name+'.txt','w')\n",
    "    \n",
    "    res = requests.get(line.strip())\n",
    "    response = res.text.encode('utf8')\n",
    "    soup = bs(response)\n",
    "    map_page = 'http://sale.591.com.tw/'+soup.select('#mapRound iframe')[0]['src']\n",
    "    map_pages = requests.get(map_page)\n",
    "    soup = bs(map_pages.text)\n",
    "    latlngs = soup.select('.propMapBarMap iframe')[0]['src'].split('&q=')[1].split('&z=')[0].split(',')\n",
    "    \n",
    "    input_file.write(latlngs[0]+','+latlngs[1]+'\\001'+'\\n'+response)\n",
    "input_file.close()\n",
    "f.close()\n",
    "#http://sale.591.com.tw/map-houseRound.html?type=2&post_id=3580915&detail=detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#練習selenium 抓HTML\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium.webdriver as webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs \n",
    "\n",
    "#print dir(webdriver)\n",
    "driver = webdriver.Chrome(\"/Users/Derek/Downloads/chromedriver\")\n",
    "f = open('591_2.txt','r')\n",
    "for line in f:\n",
    "    file_name = line.strip().split('/')[3].split('.')[0]\n",
    "    input_file = open(file_name+'.txt','w')\n",
    "    driver.get(line.strip())\n",
    "    #driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    #driver.execute_script(\"scroll(0.250);\")\n",
    "    response = driver.page_source.encode('utf8')\n",
    "    input_file.write(response)\n",
    "    time.sleep(2)\n",
    "driver.close()\n",
    "input_file.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "‪#在資料夾中逐一讀取TXT檔案裡的HTML‬\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = '/Users/Derek/Project@iii/591/'\n",
    "for file_name in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "    f = open(file_name, \"r\")\n",
    "    soup = bs(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# road:所在路段\n",
    "# latitude:緯度\n",
    "# longitude:經度\n",
    "# price:總價(萬)\n",
    "# price_per:每坪單價，若是0則需計算\n",
    "# Housing_pattern_bedroom:房(室)\n",
    "# Housing_pattern_hall:廳\n",
    "# Housing_pattern_toilet:衛\n",
    "# age:屋齡\n",
    "# floor:樓層\n",
    "# building_height:樓高\n",
    "# building_type:類型\n",
    "# elevator:電梯數目\n",
    "# orientation:座向/方位\n",
    "# area_total:總面積(總坪數)/建物登記坪數\n",
    "# main_building:主建物坪數\n",
    "# establishes_with_whole:公設坪數\n",
    "# parking_space:車位種類/有無\n",
    "# community:社區\n",
    "# building_structure:建築結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#屬性擷取(欄位不固定，失敗！！)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#res = requests.get('http://sale.591.com.tw/sale-detail-2444655.html')\n",
    "#soup = bs(res.text)\n",
    "ff = open('591_detail.txt','w')\n",
    "dir_path = '/Users/Derek/Project@iii/591/'\n",
    "for file_name in glob.glob(os.path.join(dir_path, \"*.txt\")):\n",
    "    f = open(file_name, \"r\")\n",
    "    text = open(file_name, \"r\").read()\n",
    "    soup = bs(f.read())\n",
    "#Primary Key    \n",
    "    pk = re.search('S([\\d]*)',str(soup.select('#propNav')[0].text.encode('utf8'))).group(1)\n",
    "#緯經度\n",
    "    latitude = text.split('\\001')[0].split(',')[0]\n",
    "    longitude = text.split('\\001')[0].split(',')[1]\n",
    "# 總價\n",
    "    price = re.search('([\\d\\,]*) 萬元',str(''.join(soup.select('#attr li')[0].select('span')[1].text.encode('utf8').split(',')))).group(1)\n",
    "#price = ''.join(soup.select('#attr li')[0].select('span')[1].text.split(','))\n",
    "#每坪單價\n",
    "    price_per = re.search('([\\d\\.]*)萬元/坪',str(soup.select('#attr li')[1].select('span')[1].text.encode('utf8'))).group(1)\n",
    "#格局\n",
    "    #Housing_pattern_bedroom = re.search('([\\d]*)房',str(soup.select('#attr li')[2].select('span')[1].text.encode('utf8'))).group(1)\n",
    "    #Housing_pattern_hall = re.search('([\\d]*)廳',str(soup.select('#attr li')[2].select('span')[1].text.encode('utf8'))).group(1)\n",
    "    #Housing_pattern_toilet = re.search('([\\d]*)衛',str(soup.select('#attr li')[2].select('span')[1].text.encode('utf8'))).group(1)\n",
    "#總坪數\n",
    "    area_total = re.search('([\\d\\.]*)坪',str(soup.select('#attr li')[3].select('span')[1].text.encode('utf8'))).group(1)\n",
    "#樓層/樓高\n",
    "    floor = re.search('([\\d]*)F',str(soup.select('#attr li')[4].select('span')[1].text.encode('utf8'))).group(1)\n",
    "    building_height = re.search('/([\\d]*)F',str(soup.select('#attr li')[4].select('span')[1].text.encode('utf8'))).group(1)\n",
    "#屋齡\n",
    "    age = re.search('([\\d]*)年',str(soup.select('#attr li')[5].select('span')[1].text.encode('utf8'))).group(1)\n",
    "#類型\n",
    "    building_type = soup.select('#attr li')[7].select('span')[1].text.split('/')[0].encode('utf8')\n",
    "#車位 (車位種類？)\n",
    "#parking_space = soup.select('#attr li')[8].select('span')[1].text\n",
    "    #parking_space = '1' if re.search('坪',str(soup.select('#attr li')[8].select('span')[1].text.encode('utf8'))) else '0'\n",
    "#社區\n",
    "    #community = soup.select('#attr li')[9].select('span')[1].text\n",
    "#所在路段\n",
    "    #road = soup.select('#attr li')[9].select('span')[1].text\n",
    "    road = re.search('([\\S]*)',str(soup.select('#attr')[0].select('.addr')[0].text.encode('utf8'))).group(1)\n",
    "#方位\n",
    "    #orientation = unicode(re.search('朝向：([\\S]*)',str(soup.select('#sale-other-info div')[1].text.encode('utf8'))).group(1),'utf8')\n",
    "#主建物坪數\n",
    "    #main_building = re.search('坪數說明：主建物([\\d\\.]*)坪、附屬建物([\\d\\.]*)坪、共用部分([\\d\\.]*)坪、公設比([\\d\\D]*)',str(soup.select('#sale-other-info div')[4].text.encode('utf8'))).group(1)\n",
    "#公設比\n",
    "    #establishes_with_whole = re.search('坪數說明：主建物([\\d\\.]*)坪、附屬建物([\\d\\.]*)坪、共用部分([\\d\\.]*)坪、公設比([\\d\\D]*)',str(soup.select('#sale-other-info div')[4].text.encode('utf8'))).group(4)\n",
    "\n",
    "    ff.write(pk+','+latitude+','+longitude+','+road+','+price+','+price_per \\\n",
    "            +','+age+','+floor+','+building_height+','+building_type+','+area_total)\n",
    "    ff.write('\\n')\n",
    "    #print latitude+','+longitude\n",
    "    f.close()\n",
    "ff.close()\n",
    "#print road+','+price+','+price_per+','+Housing_pattern_bedroom+','+Housing_pattern_hall+','+Housing_pattern_toilet \\\n",
    "#    +','+age+','+floor+','+building_height+','+building_type+','+orientation+','+area_total+','+main_building \\\n",
    "#    +','+establishes_with_whole+','+parking_space+','+community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "#屬性擷取(完全使用正規表示法加上判斷式，最後版本)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "\n",
    "    # ==================================\n",
    "pk = ''\n",
    "road = ''\n",
    "latitude = ''\n",
    "longitude = ''\n",
    "price = ''\n",
    "price_per =''\n",
    "Housing_pattern_bedroom = ''\n",
    "Housing_pattern_hall = ''\n",
    "Housing_pattern_toilet = ''\n",
    "age = ''\n",
    "floor = ''\n",
    "building_height = ''\n",
    "building_type = ''\n",
    "elevator = ''\n",
    "orientation = ''\n",
    "area_total = ''\n",
    "main_building = ''\n",
    "establishes_with_whole = ''\n",
    "parking_space = ''\n",
    "community = ''\n",
    "building_structure = ''\n",
    "\n",
    "\n",
    "ff = open('591xinbei_detail_final.txt','w')\n",
    "dir_path = \"591xinbei\"\n",
    "for file_name in glob.glob(os.path.join(os.getcwd(), dir_path, \"*.txt\")):\n",
    "    f = open(file_name, \"r\")\n",
    "    text = open(file_name, \"r\").read()\n",
    "    soup = bs(f.read())\n",
    "#Primary Key    \n",
    "    pk = re.search('S([\\d]*)',str(soup.select('#propNav')[0].text.encode('utf8'))).group(1)\n",
    "#緯經度\n",
    "    latitude = text.split('\\001')[0].split(',')[0]\n",
    "    longitude = text.split('\\001')[0].split(',')[1]\n",
    "    # =================主要屬性值=================\n",
    "    string = ''\n",
    "    for ele in soup.select('#attr')[0].select('li'):\n",
    "        string = string + ' ' + ' '.join(ele.text.split())\n",
    "# 總價\n",
    "    price = re.search('售金：([\\d\\,]*) 萬元',str(''.join(string.encode('utf8').split(',')))).group(1)\n",
    "#每坪單價    \n",
    "    if re.search('單價：([\\d\\.]*)萬',str(string.encode('utf8'))):\n",
    "        price_per = re.search('單價：([\\d\\.]*)萬',str(string.encode('utf8'))).group(1)  \n",
    "    else: price_per = str(round(float(price)/float(area_total),2))\n",
    "#格局\n",
    "    if re.search('格局：([\\d]*)房',str(string.encode('utf8'))):\n",
    "        Housing_pattern_bedroom = re.search('格局：([\\d]*)房',str(string.encode('utf8'))).group(1)\n",
    "    else:Housing_pattern_bedroom = ''\n",
    "        \n",
    "    if re.search('([\\d]*)廳',str(string.encode('utf8'))):\n",
    "        Housing_pattern_hall = re.search('([\\d]*)廳',str(string.encode('utf8'))).group(1)\n",
    "    else:Housing_pattern_hall = ''\n",
    "        \n",
    "    if re.search('([\\d]*)衛',str(string.encode('utf8'))):\n",
    "        Housing_pattern_toilet = re.search('([\\d]*)衛',str(string.encode('utf8'))).group(1)\n",
    "    else:Housing_pattern_toilet = ''\n",
    "#總坪數\n",
    "    if re.search('權狀坪數：([\\d\\.]*)坪',str(string.encode('utf8'))):\n",
    "        area_total = re.search('權狀坪數：([\\d\\.]*)坪',str(string.encode('utf8'))).group(1)\n",
    "    else:area_total = ''\n",
    "#樓層/樓高\n",
    "    if re.search('樓層：([\\d]*)F',str(string.encode('utf8'))):\n",
    "        floor = re.search('樓層：([\\d]*)F',str(string.encode('utf8'))).group(1)\n",
    "    elif re.search('樓層：([\\S]*)/',str(string.encode('utf8'))):\n",
    "        floor = re.search('樓層：([\\S]*)/',str(string.encode('utf8'))).group(1)\n",
    "    else: floor = ''\n",
    "        \n",
    "    if re.search('/([\\d]*)F',str(string.encode('utf8'))):\n",
    "        building_height = re.search('/([\\d]*)F',str(string.encode('utf8'))).group(1)\n",
    "    else: building_height = ''\n",
    "#屋齡\n",
    "    if re.search('屋齡：([\\d]*)年',str(string.encode('utf8'))):\n",
    "        age = re.search('屋齡：([\\d]*)年',str(string.encode('utf8'))).group(1)\n",
    "    else: age = ''\n",
    "#類型\n",
    "    if re.search('型態/類型：([\\S]*)/',str(string.encode('utf8'))):\n",
    "        building_type = re.search('型態/類型：([\\S]*)/',str(string.encode('utf8'))).group(1)\n",
    "    else: building_type =''\n",
    "#車位\n",
    "    if re.search('車位：([\\d]*) ',str(string.encode('utf8'))):\n",
    "        parking_space = re.search('車位：([\\d]*) ',str(string.encode('utf8'))).group(1)\n",
    "    elif re.search('無',str(string.encode('utf8'))):\n",
    "        parking_space = '0'\n",
    "    elif re.search('車位：([\\S]*) ',str(string.encode('utf8'))):\n",
    "        parking_space = '1'\n",
    "    else: parking_space = '-1'\n",
    "#社區\n",
    "    if re.search('社區：([\\S]*) ',str(string.encode('utf8'))):\n",
    "        community = re.search('社區：([\\S]*) ',str(string.encode('utf8'))).group(1)\n",
    "    else: community = ' '\n",
    "#所在路段\n",
    "    if re.search('地址：([\\S]*)',str(string.encode('utf8'))):\n",
    "        road = re.search('地址：([\\S]*)',str(string.encode('utf8'))).group(1)\n",
    "    else: road = ''\n",
    "    \n",
    "    # =================其他屬性值=================\n",
    "    other = ''\n",
    "    for eles in soup.select('#sale-other-info')[0].select('div'):\n",
    "        other = other + ' ' + ' '.join(eles.text.split())\n",
    "#方位\n",
    "    if re.search('朝向：([\\S]*) ',str(other.encode('utf8'))): \n",
    "        orientation = re.search('朝向：([\\S]*) ',str(other.encode('utf8'))).group(1)\n",
    "    else: orientation = ' '\n",
    "#主建物坪數\n",
    "    if re.search('主建物([\\d\\.]*)坪',str(other.encode('utf8'))): \n",
    "        main_building = re.search('主建物([\\d\\.]*)坪',str(other.encode('utf8'))).group(1)\n",
    "    else: main_building = ' '\n",
    "#公設比\n",
    "    if re.search('公設比([\\d\\S]*) ',str(other.encode('utf8'))):\n",
    "        tmp = re.search('公設比([\\d]*)',str(other.encode('utf8'))).group(1)\n",
    "        establishes_with_whole = str(float(tmp)/100)\n",
    "    else: establishes_with_whole = ' '\n",
    "        \n",
    "                \n",
    "    ff.write(pk+','+road+','+latitude+','+longitude+','+price+','+price_per+','+Housing_pattern_bedroom \\\n",
    "       +','+Housing_pattern_hall+','+Housing_pattern_toilet+','+age+','+floor+','+building_height \\\n",
    "        +','+building_type+','+elevator+','+orientation+','+area_total+','+main_building \\\n",
    "       +','+establishes_with_whole+','+parking_space+','+community+','+building_structure)\n",
    "    ff.write('\\n')\n",
    "    f.close()\n",
    "ff.close()\n",
    "\n",
    " \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#練習 多執行緒\n",
    "from datetime import datetime\n",
    "import Queue\n",
    "from threading import Thread\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "startTime = datatime.now()\n",
    "\n",
    "q = Queue.LifoQueue()\n",
    "\n",
    "url = 'http://www.craigaddyman.com/sitemap.xml'\n",
    "\n",
    "r = requests.get(url, timeout=5)\n",
    "data = r.text\n",
    "soup = bs(data.encode('utf8'))\n",
    "\n",
    "def sitemap_parser(soup):\n",
    "    for url in soup.findAll('loc'):\n",
    "        q.put(url.text)\n",
    "sitemap_parser(soup)\n",
    "\n",
    "def grab_data_from_queue():\n",
    "    while not q.empty():\n",
    "        url = q.get()\n",
    "        r = requests.get(url.strip())\n",
    "        print r.status_code, r.url\n",
    "        q.task_done()\n",
    "\n",
    "for i in range(10):\n",
    "    t1 = Thread(target = grab_data_from_queue)\n",
    "    t1.start()\n",
    "    \n",
    "q.join()\n",
    "\n",
    "print datetime.now()-startTime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#練習 多執行緒\n",
    "from datetime import datetime\n",
    "import Queue\n",
    "from threading import Thread\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "startTime = datatime.now()\n",
    "\n",
    "q = Queue.LifoQueue()\n",
    "\n",
    "dir = '591xinbei'\n",
    "f = open('591xinbei_URL.txt','r')\n",
    "\n",
    "for line in f:\n",
    "    file_name = line.strip().split('/')[3].split('.')[0].split('-')[2]\n",
    "    input_file = open(dir+'/'+file_name+'.txt','w')\n",
    "    \n",
    "    url = line.strip()\n",
    "\n",
    "    r = requests.get(url, timeout=5)\n",
    "    data = r.text\n",
    "    soup = bs(data.encode('utf8'))\n",
    "\n",
    "    def sitemap_parser(soup):\n",
    "        for url in soup.findAll('loc'):\n",
    "            q.put(url.text)\n",
    "    sitemap_parser(soup)\n",
    "\n",
    "def grab_data_from_queue():\n",
    "    while not q.empty():\n",
    "        url = q.get()\n",
    "        r = requests.get(url.strip())\n",
    "        print r.status_code, r.url\n",
    "        q.task_done()\n",
    "\n",
    "for i in range(10):\n",
    "    t1 = Thread(target = grab_data_from_queue)\n",
    "    t1.start()\n",
    "    \n",
    "q.join()\n",
    "\n",
    "print datetime.now()-startTime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
